{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data & Helper Functions"
      ],
      "metadata": {
        "id": "uHWFN-7hQmDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing some helper functions that would be used later during training"
      ],
      "metadata": {
        "id": "zrTWvHSrQoo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Some helper functions for PyTorch, including:\n",
        "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
        "    - msr_init: net parameter initialization.\n",
        "    - progress_bar: progress bar mimic xlua.progress.\n",
        "'''\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# fixing random seeds\n",
        "random_seed = 6\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "_, term_width = shutil.get_terminal_size()\n",
        "term_width = int(term_width)\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f\n"
      ],
      "metadata": {
        "id": "4fD5dE5ltd7F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "RHrqS8oYQvZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define our model - DLA class. In our experiments, we use the DLA (https://arxiv.org/abs/1707.06484) architecture with ZigZag-aligned modifications, specifically with a modified first layer."
      ],
      "metadata": {
        "id": "otflIWYoQyLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Simplified version of DLA in PyTorch.\n",
        "Note this implementation is not identical to the original paper version.\n",
        "But it seems works fine.\n",
        "See dla.py for the original paper version.\n",
        "Reference:\n",
        "    Deep Layer Aggregation. https://arxiv.org/abs/1707.06484\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Root(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
        "        super(Root, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size,\n",
        "            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, xs):\n",
        "        x = torch.cat(xs, 1)\n",
        "        out = F.relu(self.bn(self.conv(x)))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Tree(nn.Module):\n",
        "    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n",
        "        super(Tree, self).__init__()\n",
        "        self.root = Root(2*out_channels, out_channels)\n",
        "        if level == 1:\n",
        "            self.left_tree = block(in_channels, out_channels, stride=stride)\n",
        "            self.right_tree = block(out_channels, out_channels, stride=1)\n",
        "        else:\n",
        "            self.left_tree = Tree(block, in_channels,\n",
        "                                  out_channels, level=level-1, stride=stride)\n",
        "            self.right_tree = Tree(block, out_channels,\n",
        "                                   out_channels, level=level-1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.left_tree(x)\n",
        "        out2 = self.right_tree(out1)\n",
        "        out = self.root([out1, out2])\n",
        "        return out\n",
        "\n",
        "class F1(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, num_classes=10):\n",
        "        super(F1, self).__init__()\n",
        "        self.base = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.layer3 = Tree(block,  32,  64, level=1, stride=1)\n",
        "        self.layer4 = Tree(block,  64, 128, level=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.base(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        return out\n",
        "\n",
        "class F2(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, num_classes=10):\n",
        "        super(F2, self).__init__()\n",
        "\n",
        "        self.y_hat_fc = nn.Sequential(\n",
        "            nn.Linear(10, 128),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n",
        "        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n",
        "        self.linear = nn.Linear(512, num_classes + 128)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + self.y_hat_fc(y)[..., None, None]\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        z = F.avg_pool2d(out, 4)\n",
        "        z = z.view(z.size(0), -1)\n",
        "        z = self.linear(z)\n",
        "        return z[:, :10], z[:, 10:]\n",
        "\n",
        "class DLA(nn.Module):\n",
        "    def __init__(self, block=BasicBlock, num_classes=10):\n",
        "        super(DLA, self).__init__()\n",
        "        self.f1 = F1(block, num_classes)\n",
        "        self.f2 = F2(block, num_classes)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        z = self.f1(x)\n",
        "        y_pred, z_pred  = self.f2(z, y)\n",
        "        return y_pred, z_pred"
      ],
      "metadata": {
        "id": "Lk3uJKVWthYN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Training"
      ],
      "metadata": {
        "id": "S3KKFjwlROAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define the optimization parameters and the functions for **train** and **test**. As described in the paper, the first inference of our method is performed with a \"blank\" additional input, while the second inference incorporates the class labels."
      ],
      "metadata": {
        "id": "JXEnOBl1RN98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import tqdm\n",
        "\n",
        "class Args:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.lr = 0.001\n",
        "    self.resume = False\n",
        "    self.checkpoint = \"cifar_iter\"\n",
        "    self.batch_size = 56\n",
        "\n",
        "args = Args()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Model\n",
        "device = \"cuda\"\n",
        "print('==> Building model..')\n",
        "net = DLA().to(device)\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "\n",
        "batch_size = 64\n",
        "num_workers = 64\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=args.batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "if args.resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load(f'./checkpoint/{args.checkpoint}.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    t = tqdm.trange(len(trainset) // args.batch_size + 1, desc='Current Loss = ', leave=True)\n",
        "\n",
        "    for _, (batch_idx, (inputs, targets)) in zip(t, enumerate(trainloader)):\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        batch_size, _, H, W = inputs.shape\n",
        "\n",
        "        x1 = torch.cat([inputs], dim=1)\n",
        "\n",
        "        inputs = torch.cat([x1], dim=0)\n",
        "        targets = torch.cat([targets], dim=0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        mask = torch.rand(1) > 0.8\n",
        "\n",
        "        # creating \"blank\" inputs for the first inferense\n",
        "        # in this case, it's uniform distribution over 10 classes\n",
        "        y_0 = F.one_hot(targets, num_classes=10).float() if mask else torch.ones(batch_size, 10).to(device) * 0.1\n",
        "        z = net.module.f1(inputs)\n",
        "\n",
        "        # first inference with \"blank\" input\n",
        "        y_1, z1 = net.module.f2(z, y_0)\n",
        "\n",
        "        # creating additional input for the second inference\n",
        "        y_2, z2 = net.module.f2(z + z1[..., None, None], y_1.softmax(-1))\n",
        "\n",
        "        loss_supervised_1 = criterion(y_1, targets)\n",
        "        loss_supervised_2 = criterion(y_2, targets)\n",
        "\n",
        "        loss = loss_supervised_1 + loss_supervised_2\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = y_2.max(1)\n",
        "        if not mask:\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if total != 0:\n",
        "            t.set_description(f\"Epoch {epoch} Current Loss = {round(100.*correct/total, 3)}\", refresh=True)\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size, _, H, W = inputs.shape\n",
        "            inputs = torch.cat([inputs], dim=1)\n",
        "\n",
        "            # creating \"blank\" inputs for the first inferense\n",
        "            # in this case, it's uniform distribution over 10 classes\n",
        "            y_0 = torch.ones(batch_size, 10).to(device) * 0.1\n",
        "\n",
        "            z = net.module.f1(inputs)\n",
        "\n",
        "             # first inference with \"blank\" input\n",
        "            y_1, z1 = net.module.f2(z, y_0)\n",
        "\n",
        "            # creating additional input for the second inference\n",
        "            y_2, z2 = net.module.f2(z + z1[..., None, None], y_1.softmax(-1))\n",
        "\n",
        "            loss = criterion(y_2, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = y_1.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, f'./checkpoint/{args.checkpoint}.pth')\n",
        "        best_acc = acc\n",
        "\n",
        "    print(\"BEST ACCURACY: \", best_acc, \"Current Accuracy: \", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt6RJd2UvqIp",
        "outputId": "79845ad9-392e-442f-a165-9787236fb0cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "==> Building model..\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "1xmjL-DVSquZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell trains the model based on ZigZag. For convenience, a pretrained checkpoint is provided in the following section."
      ],
      "metadata": {
        "id": "tO7GhcnaSsl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(start_epoch, start_epoch + 70):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] *= 0.1\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + 10):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "jTnAQHByvsxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "mGVqaP88v688"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model\n",
        "device = \"cuda\"\n",
        "print('==> Building model..')\n",
        "net = DLA().to(device)\n",
        "\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gbWphgpvufE",
        "outputId": "53c9dad1-36a4-440f-d16f-0d27cfcc79f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download pretrained weights"
      ],
      "metadata": {
        "id": "M0cTXnZ5UMkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1VeGGbAdfBvVyOybZwbQ7bSC24Bbli4X9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMnzXOJdwMKG",
        "outputId": "b7ce3e52-7690-4e3f-960c-61a97c3c55b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1VeGGbAdfBvVyOybZwbQ7bSC24Bbli4X9\n",
            "From (redirected): https://drive.google.com/uc?id=1VeGGbAdfBvVyOybZwbQ7bSC24Bbli4X9&confirm=t&uuid=952e5a9f-1ed3-4608-8aa6-d5551487ab8d\n",
            "To: /content/cifar_iter.pth\n",
            "100% 61.0M/61.0M [00:00<00:00, 147MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"./cifar_iter.pth\"\n",
        "\n",
        "# Load checkpoint.\n",
        "print('==> Resuming from checkpoint..')\n",
        "checkpoint = torch.load(checkpoint)\n",
        "net.load_state_dict(checkpoint['net'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5wZ0NqWvwM2",
        "outputId": "e03d2ee8-dc17-415e-eea2-be0f774af91b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Resuming from checkpoint..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-52848c702b5e>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noisy Inputs Evaluation\n",
        "\n",
        "In this subsection, we apply a substantial amount of Gaussian noise to the input images. We observe a significant drop in performance for the original (non-optimized) model, whereas the ITTT-optimized model performs considerably better."
      ],
      "metadata": {
        "id": "OY9TxEmIUqAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, std, mean=0.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    AddGaussianNoise(0.5)\n",
        "])\n",
        "\n",
        "ood_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=64, shuffle=False, num_workers=num_workers, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vR1dTE8Upfp",
        "outputId": "5fd2ab9c-c274-4ead-cd6e-c5be8cbacadc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla Model\n",
        "\n",
        "The vanilla model achieves only around 30% accuracy on noisy inputs, compared to over 90% on clean data."
      ],
      "metadata": {
        "id": "bbnt8a89VOr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "def test_vanilla(net, dataloader):\n",
        "    global best_acc\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in tqdm.tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            batch_size, _, H, W = inputs.shape\n",
        "\n",
        "            inputs = torch.cat([inputs], dim=1)\n",
        "\n",
        "            y_0 = torch.ones(batch_size, 10).to(device) * 0.1\n",
        "\n",
        "            z = net.module.f1(inputs)\n",
        "            y_1, z1 = net.module.f2(z, y_0)\n",
        "            y_2, z2 = net.module.f2(z + z1[..., None, None], y_1.softmax(-1))\n",
        "\n",
        "            loss = criterion(y_2, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = y_1.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = round(100.*correct/total, 2)\n",
        "    print(f\"\\nAccuracy: {acc}%\")"
      ],
      "metadata": {
        "id": "aCVI-WJVVQfp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vanilla(net, ood_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcKS-crKVRG3",
        "outputId": "01cdea6c-af2f-4e61-8931-8c7cdfe97655"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 156/156 [00:26<00:00,  5.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 31.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ITTT Optimized Model\n",
        "\n",
        "The model optimized with Idempotent Test-Time Training achieves approximately 40% higher accuracy on noisy data."
      ],
      "metadata": {
        "id": "6yyjf6BkYyQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def js_div(p, q):\n",
        "    \"\"\"Function that computes distance between two predictions\"\"\"\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (F.kl_div(torch.log(p), m, reduction='batchmean') +\n",
        "                  F.kl_div(torch.log(q), m, reduction='batchmean'))\n",
        "\n",
        "def ttt_one_instance(x, f_ttt, f, optimizer, n_steps, y, n_classes=10):\n",
        "  \"\"\"Function that runs test-time training on one batch 'x'\"\"\"\n",
        "\n",
        "  f_ttt.load_state_dict(f.state_dict())  # reset f_ttt to f\n",
        "  f_ttt.train()\n",
        "  for step in range(n_steps):\n",
        "    y_0 = torch.ones(batch_size, 10).to(device) * 0.1\n",
        "\n",
        "    z = f_ttt.f1(x)\n",
        "    y_1, z1 = f_ttt.f2(z, y_0)\n",
        "    y_2, z2 = f_ttt.f2(z + z1[..., None, None], y_1.softmax(-1))\n",
        "\n",
        "    loss_unsupervised_y = js_div(y_1.softmax(-1), y_2.softmax(-1))\n",
        "    loss_unsupervised_z = (z1 - z2).pow(2).mean()\n",
        "    loss = loss_unsupervised_y\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  return y_1, y_2\n",
        "\n",
        "\n",
        "def ttt(f, test_loader, n_steps, lr):\n",
        "  \"\"\"Running test-time training over the whole test dataloader\"\"\"\n",
        "\n",
        "  f_ttt = deepcopy(f)\n",
        "  f.eval()\n",
        "  optimizer = optim.Adam(f_ttt.parameters(), lr=lr)\n",
        "  test_loss_1, correct_1 = 0, 0\n",
        "  test_loss_2, correct_2 = 0, 0\n",
        "\n",
        "  for ind, (data, target) in tqdm.tqdm(enumerate(test_loader), total=len(test_loader)):\n",
        "    x, y = data.to(device), target.to(device)\n",
        "    y_hat_1, y_hat_2 = ttt_one_instance(x, f_ttt, f, optimizer, n_steps, y)\n",
        "\n",
        "    test_loss_1 += F.nll_loss(y_hat_1.log(), y, size_average=False).item()\n",
        "    test_loss_2 += F.nll_loss(y_hat_2.log(), y, size_average=False).item()\n",
        "\n",
        "    pred_1 = y_hat_1.data.max(1, keepdim=True)[1]\n",
        "    pred_2 = y_hat_2.data.max(1, keepdim=True)[1]\n",
        "\n",
        "    correct_1 += pred_1.eq(y.data.view_as(pred_1)).sum()\n",
        "    correct_2 += pred_2.eq(y.data.view_as(pred_2)).sum()\n",
        "\n",
        "  acc = round(100. * int(correct_2) / len(test_loader.dataset), 2)\n",
        "  print(f\"\\nAccuracy: {acc}%\")"
      ],
      "metadata": {
        "id": "AhSWhtQ6wJN0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "ttt(net.module, ood_loader, n_steps=1, lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJPKqOoi0vcQ",
        "outputId": "ea261c13-ea76-4ee7-dffa-5264931e6ca5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/156 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "100%|██████████| 156/156 [01:12<00:00,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 68.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}